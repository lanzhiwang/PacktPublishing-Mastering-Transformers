{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v61w8HXuZrLH"
      },
      "source": [
        "# 3. Autoencoding Language Models\n",
        "自动编码语言模型\n",
        "\n",
        "##### BERT: As one of Autoencoding Language Models\n",
        "\n",
        "## Preparation for Google Collab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pfC5vfOTSob4",
        "outputId": "cc47fbf6-a57f-4ab7-a220-141d14a2360f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "/content\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "#  挂载 google 云盘\n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "print(os.getcwd())  # /content\n",
        "\n",
        "# print(os.listdir(\"/content/drive/MyDrive/\"))\n",
        "\n",
        "# print(os.listdir(\"/content/drive/MyDrive/Colab Notebooks\"))\n",
        "\n",
        "# if os.getcwd() != \"/content/drive/MyDrive\":\n",
        "#     os.chdir(\"/content/drive/MyDrive\")\n",
        "\n",
        "# print(os.getcwd())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p_tTymgLSoe-",
        "outputId": "48d468cb-c51b-4adc-c22a-21addd7d25e5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: scikit-learn==1.6.1 in /usr/local/lib/python3.11/dist-packages (from -r /content/drive/MyDrive/requirements.txt (line 2)) (1.6.1)\n",
            "Requirement already satisfied: py3nvml==0.2.7 in /usr/local/lib/python3.11/dist-packages (from -r /content/drive/MyDrive/requirements.txt (line 3)) (0.2.7)\n",
            "Requirement already satisfied: datasets==3.3.2 in /usr/local/lib/python3.11/dist-packages (from -r /content/drive/MyDrive/requirements.txt (line 4)) (3.3.2)\n",
            "Requirement already satisfied: evaluate==0.4.3 in /usr/local/lib/python3.11/dist-packages (from -r /content/drive/MyDrive/requirements.txt (line 5)) (0.4.3)\n",
            "Requirement already satisfied: huggingface-hub==0.28.1 in /usr/local/lib/python3.11/dist-packages (from -r /content/drive/MyDrive/requirements.txt (line 6)) (0.28.1)\n",
            "Requirement already satisfied: black[jupyter] in /usr/local/lib/python3.11/dist-packages (from -r /content/drive/MyDrive/requirements.txt (line 1)) (25.1.0)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.6.1->-r /content/drive/MyDrive/requirements.txt (line 2)) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.6.1->-r /content/drive/MyDrive/requirements.txt (line 2)) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.6.1->-r /content/drive/MyDrive/requirements.txt (line 2)) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.6.1->-r /content/drive/MyDrive/requirements.txt (line 2)) (3.5.0)\n",
            "Requirement already satisfied: xmltodict in /usr/local/lib/python3.11/dist-packages (from py3nvml==0.2.7->-r /content/drive/MyDrive/requirements.txt (line 3)) (0.14.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets==3.3.2->-r /content/drive/MyDrive/requirements.txt (line 4)) (3.17.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets==3.3.2->-r /content/drive/MyDrive/requirements.txt (line 4)) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets==3.3.2->-r /content/drive/MyDrive/requirements.txt (line 4)) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets==3.3.2->-r /content/drive/MyDrive/requirements.txt (line 4)) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets==3.3.2->-r /content/drive/MyDrive/requirements.txt (line 4)) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets==3.3.2->-r /content/drive/MyDrive/requirements.txt (line 4)) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets==3.3.2->-r /content/drive/MyDrive/requirements.txt (line 4)) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets==3.3.2->-r /content/drive/MyDrive/requirements.txt (line 4)) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets==3.3.2->-r /content/drive/MyDrive/requirements.txt (line 4)) (2024.10.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets==3.3.2->-r /content/drive/MyDrive/requirements.txt (line 4)) (3.11.12)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets==3.3.2->-r /content/drive/MyDrive/requirements.txt (line 4)) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets==3.3.2->-r /content/drive/MyDrive/requirements.txt (line 4)) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub==0.28.1->-r /content/drive/MyDrive/requirements.txt (line 6)) (4.12.2)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from black[jupyter]->-r /content/drive/MyDrive/requirements.txt (line 1)) (8.1.8)\n",
            "Requirement already satisfied: mypy-extensions>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from black[jupyter]->-r /content/drive/MyDrive/requirements.txt (line 1)) (1.0.0)\n",
            "Requirement already satisfied: pathspec>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from black[jupyter]->-r /content/drive/MyDrive/requirements.txt (line 1)) (0.12.1)\n",
            "Requirement already satisfied: platformdirs>=2 in /usr/local/lib/python3.11/dist-packages (from black[jupyter]->-r /content/drive/MyDrive/requirements.txt (line 1)) (4.3.6)\n",
            "Requirement already satisfied: ipython>=7.8.0 in /usr/local/lib/python3.11/dist-packages (from black[jupyter]->-r /content/drive/MyDrive/requirements.txt (line 1)) (7.34.0)\n",
            "Requirement already satisfied: tokenize-rt>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from black[jupyter]->-r /content/drive/MyDrive/requirements.txt (line 1)) (6.1.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==3.3.2->-r /content/drive/MyDrive/requirements.txt (line 4)) (2.4.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==3.3.2->-r /content/drive/MyDrive/requirements.txt (line 4)) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==3.3.2->-r /content/drive/MyDrive/requirements.txt (line 4)) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==3.3.2->-r /content/drive/MyDrive/requirements.txt (line 4)) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==3.3.2->-r /content/drive/MyDrive/requirements.txt (line 4)) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==3.3.2->-r /content/drive/MyDrive/requirements.txt (line 4)) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==3.3.2->-r /content/drive/MyDrive/requirements.txt (line 4)) (1.18.3)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.11/dist-packages (from ipython>=7.8.0->black[jupyter]->-r /content/drive/MyDrive/requirements.txt (line 1)) (75.1.0)\n",
            "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.11/dist-packages (from ipython>=7.8.0->black[jupyter]->-r /content/drive/MyDrive/requirements.txt (line 1)) (0.19.2)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from ipython>=7.8.0->black[jupyter]->-r /content/drive/MyDrive/requirements.txt (line 1)) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.11/dist-packages (from ipython>=7.8.0->black[jupyter]->-r /content/drive/MyDrive/requirements.txt (line 1)) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.11/dist-packages (from ipython>=7.8.0->black[jupyter]->-r /content/drive/MyDrive/requirements.txt (line 1)) (5.7.1)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from ipython>=7.8.0->black[jupyter]->-r /content/drive/MyDrive/requirements.txt (line 1)) (3.0.50)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.11/dist-packages (from ipython>=7.8.0->black[jupyter]->-r /content/drive/MyDrive/requirements.txt (line 1)) (2.18.0)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.11/dist-packages (from ipython>=7.8.0->black[jupyter]->-r /content/drive/MyDrive/requirements.txt (line 1)) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.11/dist-packages (from ipython>=7.8.0->black[jupyter]->-r /content/drive/MyDrive/requirements.txt (line 1)) (0.1.7)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.11/dist-packages (from ipython>=7.8.0->black[jupyter]->-r /content/drive/MyDrive/requirements.txt (line 1)) (4.9.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets==3.3.2->-r /content/drive/MyDrive/requirements.txt (line 4)) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets==3.3.2->-r /content/drive/MyDrive/requirements.txt (line 4)) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets==3.3.2->-r /content/drive/MyDrive/requirements.txt (line 4)) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets==3.3.2->-r /content/drive/MyDrive/requirements.txt (line 4)) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets==3.3.2->-r /content/drive/MyDrive/requirements.txt (line 4)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets==3.3.2->-r /content/drive/MyDrive/requirements.txt (line 4)) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets==3.3.2->-r /content/drive/MyDrive/requirements.txt (line 4)) (2025.1)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.11/dist-packages (from jedi>=0.16->ipython>=7.8.0->black[jupyter]->-r /content/drive/MyDrive/requirements.txt (line 1)) (0.8.4)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.11/dist-packages (from pexpect>4.3->ipython>=7.8.0->black[jupyter]->-r /content/drive/MyDrive/requirements.txt (line 1)) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=7.8.0->black[jupyter]->-r /content/drive/MyDrive/requirements.txt (line 1)) (0.2.13)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets==3.3.2->-r /content/drive/MyDrive/requirements.txt (line 4)) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "# 提前将 requirements.txt 放在 google 云盘上\n",
        "!pip install -r /content/drive/MyDrive/requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lzctm7hGSoh-",
        "outputId": "b3949137-6adf-4c98-e458-eac950835672"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/ch03a\n"
          ]
        }
      ],
      "source": [
        "subdir = \"ch03a\"\n",
        "work_path = \"/content/drive/MyDrive/\" + subdir\n",
        "if not os.path.exists(work_path):\n",
        "    os.mkdir(work_path)\n",
        "os.chdir(work_path)\n",
        "print(os.getcwd())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m6oNSJxwSolR",
        "outputId": "42e4c795-6b40-4488-cd51-f41e7d1e3325"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "tree is already the newest version (2.0.2-1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 23 not upgraded.\n",
            "\u001b[01;34m./\u001b[0m\n",
            "└── \u001b[00mIMDB.csv\u001b[0m\n",
            "\n",
            "0 directories, 1 file\n"
          ]
        }
      ],
      "source": [
        "!apt-get install tree && tree -a \"./\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "io3A3NJ3srXu"
      },
      "source": [
        "## 1 准备数据\n",
        "\n",
        "1. 将原始数据文件 (IMDB.csv) 重新组织为 corpus.txt 文件\n",
        "2. 利用 corpus.txt 数据进行分词, 生成 vocab.txt\n",
        "3. 利用 vocab.txt 训练分词器\n",
        "4. 利用 corpus.txt 生成数据集\n",
        "5. 生成 PyTorch DataLoaders\n",
        "6. 训练参数\n",
        "7. 模型配置\n",
        "8. 模型结构\n",
        "9. 训练\n",
        "10. 保存模型"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ll2FxEGosqyt",
        "outputId": "9b93d8d1-1d72-432d-b1df-711a2599524e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "review,sentiment\n",
            "\"One of the other reviewers has mentioned that after watching just hooked.\",positive\n",
            "\"A wonderful little production.\",positive\n",
            "\"I thought this was a wonderful way to spend time on a too hot summer weekend\",positive\n",
            "\"his parents are fighting all the time.\",negative"
          ]
        }
      ],
      "source": [
        "!cat IMDB.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "id": "sJbh3P-e3g0S",
        "outputId": "a4a987d7-9dc0-4226-8ab1-c2b8a2087562"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"imdb_df\",\n  \"rows\": 4,\n  \"fields\": [\n    {\n      \"column\": \"review\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"A wonderful little production.\",\n          \"his parents are fighting all the time.\",\n          \"One of the other reviewers has mentioned that after watching just hooked.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sentiment\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"negative\",\n          \"positive\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe",
              "variable_name": "imdb_df"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-3bee425c-31c2-4d04-99c3-7197529d36bb\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>One of the other reviewers has mentioned that ...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>A wonderful little production.</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>I thought this was a wonderful way to spend ti...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>his parents are fighting all the time.</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3bee425c-31c2-4d04-99c3-7197529d36bb')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-3bee425c-31c2-4d04-99c3-7197529d36bb button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-3bee425c-31c2-4d04-99c3-7197529d36bb');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-12d15ac7-64d0-4935-90eb-8dbd59344db1\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-12d15ac7-64d0-4935-90eb-8dbd59344db1')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-12d15ac7-64d0-4935-90eb-8dbd59344db1 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_163aea7c-8e33-4284-901d-4d362eb9ec90\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('imdb_df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_163aea7c-8e33-4284-901d-4d362eb9ec90 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('imdb_df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "                                              review sentiment\n",
              "0  One of the other reviewers has mentioned that ...  positive\n",
              "1                     A wonderful little production.  positive\n",
              "2  I thought this was a wonderful way to spend ti...  positive\n",
              "3             his parents are fighting all the time.  negative"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "imdb_df = pd.read_csv(\"IMDB.csv\")\n",
        "imdb_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "CLsn_uEKROOd",
        "outputId": "7f627c47-0efe-4754-c2b3-e65a865a7d83"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "' One of the other reviewers has mentioned that ...\\n                    A wonderful little production.\\n I thought this was a wonderful way to spend ti...\\n            his parents are fighting all the time.'"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "reviews = imdb_df.review.to_string(index=None)\n",
        "reviews"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "rtZ8gILwRLDb"
      },
      "outputs": [],
      "source": [
        "with open(\"corpus.txt\", \"w\") as f:\n",
        "    f.writelines(reviews)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NEflbx-5SiKY",
        "outputId": "a5378c80-0686-4f35-b786-141b2cef1151"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[01;34m./\u001b[0m\n",
            "├── \u001b[00mcorpus.txt\u001b[0m\n",
            "└── \u001b[00mIMDB.csv\u001b[0m\n",
            "\n",
            "0 directories, 2 files\n"
          ]
        }
      ],
      "source": [
        "!tree -a \"./\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uL2YZZDUSkiS",
        "outputId": "08f2f804-ce2e-45cc-9dd1-e459d8bf64b0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " One of the other reviewers has mentioned that ...\n",
            "                    A wonderful little production.\n",
            " I thought this was a wonderful way to spend ti...\n",
            "            his parents are fighting all the time."
          ]
        }
      ],
      "source": [
        "!cat \"./corpus.txt\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mehkjxFus-by"
      },
      "source": [
        "## 2 tokenizers 分词\n",
        "\n",
        "1. 将原始数据文件 (IMDB.csv) 重新组织为 corpus.txt 文件\n",
        "2. 利用 corpus.txt 数据进行分词, 生成 vocab.txt\n",
        "3. 利用 vocab.txt 训练分词器\n",
        "4. 利用 corpus.txt 生成数据集\n",
        "5. 生成 PyTorch DataLoaders\n",
        "6. 训练参数\n",
        "7. 模型配置\n",
        "8. 模型结构\n",
        "9. 训练\n",
        "10. 保存模型"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JQRzoQKS4SA2",
        "outputId": "fdfd39c9-3469-4564-c9e5-95d682c32546"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Tokenizer(vocabulary_size=0, model=BertWordPiece, unk_token=[UNK], sep_token=[SEP], cls_token=[CLS], pad_token=[PAD], mask_token=[MASK], clean_text=True, handle_chinese_chars=True, strip_accents=None, lowercase=True, wordpieces_prefix=##)"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from tokenizers import BertWordPieceTokenizer\n",
        "\n",
        "bert_wordpiece_tokenizer = BertWordPieceTokenizer()\n",
        "bert_wordpiece_tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "HBh_lrPlLkpA"
      },
      "outputs": [],
      "source": [
        "bert_wordpiece_tokenizer.train(\"corpus.txt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W1I-zmMaS3vt",
        "outputId": "f534b556-41a4-408b-da4e-a1bf142c8a3b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Tokenizer(vocabulary_size=63, model=BertWordPiece, unk_token=[UNK], sep_token=[SEP], cls_token=[CLS], pad_token=[PAD], mask_token=[MASK], clean_text=True, handle_chinese_chars=True, strip_accents=None, lowercase=True, wordpieces_prefix=##)"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "bert_wordpiece_tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "datICiehV7kv",
        "outputId": "21ab3c85-d04e-441b-c545-8aceb9841ac4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'[CLS]': 2,\n",
              " '##y': 44,\n",
              " '##w': 33,\n",
              " 'd': 8,\n",
              " '##r': 34,\n",
              " '[PAD]': 0,\n",
              " '##e': 26,\n",
              " 't': 21,\n",
              " '[MASK]': 4,\n",
              " '##a': 41,\n",
              " '##v': 32,\n",
              " '##en': 49,\n",
              " 'c': 7,\n",
              " '.': 5,\n",
              " 'won': 53,\n",
              " '##on': 48,\n",
              " 's': 20,\n",
              " '##c': 45,\n",
              " '##d': 31,\n",
              " '##er': 47,\n",
              " 'h': 12,\n",
              " '##der': 55,\n",
              " '##i': 29,\n",
              " 'g': 11,\n",
              " 'n': 16,\n",
              " 'v': 23,\n",
              " 'wa': 52,\n",
              " 'ti': 51,\n",
              " 'w': 24,\n",
              " '##o': 30,\n",
              " '##gh': 57,\n",
              " 'wonder': 60,\n",
              " 'e': 9,\n",
              " '##m': 43,\n",
              " 'o': 17,\n",
              " '[UNK]': 1,\n",
              " '##g': 40,\n",
              " '##p': 42,\n",
              " 'y': 25,\n",
              " '##s': 35,\n",
              " 'm': 15,\n",
              " '##ti': 50,\n",
              " 'i': 13,\n",
              " '##f': 36,\n",
              " '##tion': 59,\n",
              " '##l': 38,\n",
              " '##n': 27,\n",
              " 'u': 22,\n",
              " 'th': 46,\n",
              " 'p': 18,\n",
              " 'a': 6,\n",
              " '##h': 39,\n",
              " '##fu': 56,\n",
              " '##ful': 61,\n",
              " '##u': 37,\n",
              " 'wonderful': 62,\n",
              " 'r': 19,\n",
              " '[SEP]': 3,\n",
              " '##t': 28,\n",
              " 'l': 14,\n",
              " 'the': 58,\n",
              " '##is': 54,\n",
              " 'f': 10}"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "bert_wordpiece_tokenizer.get_vocab()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "lIID_rquXa37"
      },
      "outputs": [],
      "source": [
        "!mkdir tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yGW0vRc1NR-C",
        "outputId": "ddc1f6ce-886c-47bb-e8c6-827899bf3852"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "total 0\n"
          ]
        }
      ],
      "source": [
        "!ls -al tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "buBdhU5RLkpA",
        "outputId": "8323a2e2-1cee-4b26-fb54-b143ccea671a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['tokenizer/vocab.txt']"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "bert_wordpiece_tokenizer.save_model(\"tokenizer\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uwnKmAfQNYla",
        "outputId": "3fd5deb2-ae7f-4740-9264-0a66534de3d8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[01;34m./\u001b[0m\n",
            "├── \u001b[00mcorpus.txt\u001b[0m\n",
            "├── \u001b[00mIMDB.csv\u001b[0m\n",
            "└── \u001b[01;34mtokenizer\u001b[0m\n",
            "    └── \u001b[00mvocab.txt\u001b[0m\n",
            "\n",
            "1 directory, 3 files\n"
          ]
        }
      ],
      "source": [
        "!tree -a \"./\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LoopabQSTKxD",
        "outputId": "0746c39d-5074-475f-ad71-374006ee648d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[PAD]\n",
            "[UNK]\n",
            "[CLS]\n",
            "[SEP]\n",
            "[MASK]\n",
            ".\n",
            "a\n",
            "c\n",
            "d\n",
            "e\n",
            "f\n",
            "g\n",
            "h\n",
            "i\n",
            "l\n",
            "m\n",
            "n\n",
            "o\n",
            "p\n",
            "r\n",
            "s\n",
            "t\n",
            "u\n",
            "v\n",
            "w\n",
            "y\n",
            "##e\n",
            "##n\n",
            "##t\n",
            "##i\n",
            "##o\n",
            "##d\n",
            "##v\n",
            "##w\n",
            "##r\n",
            "##s\n",
            "##f\n",
            "##u\n",
            "##l\n",
            "##h\n",
            "##g\n",
            "##a\n",
            "##p\n",
            "##m\n",
            "##y\n",
            "##c\n",
            "th\n",
            "##er\n",
            "##on\n",
            "##en\n",
            "##ti\n",
            "ti\n",
            "wa\n",
            "won\n",
            "##is\n",
            "##der\n",
            "##fu\n",
            "##gh\n",
            "the\n",
            "##tion\n",
            "wonder\n",
            "##ful\n",
            "wonderful\n"
          ]
        }
      ],
      "source": [
        "!cat \"./tokenizer/vocab.txt\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-zxTowtYt1Tq"
      },
      "source": [
        "##### 验证分词"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JF5_gklbXeeu",
        "outputId": "92981dc2-ee2b-4928-9158-b0f135a45078"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Tokenizer(vocabulary_size=63, model=BertWordPiece, unk_token=[UNK], sep_token=[SEP], cls_token=[CLS], pad_token=[PAD], mask_token=[MASK], clean_text=True, handle_chinese_chars=True, strip_accents=None, lowercase=True, wordpieces_prefix=##)"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer1 = BertWordPieceTokenizer.from_file(\"tokenizer/vocab.txt\")\n",
        "tokenizer1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R-ANSWWKXz29",
        "outputId": "12b56315-0906-498d-cd08-52dba98d50ec"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Encoding(num_tokens=12, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenized_sentence = tokenizer1.encode(\"Oh it works just fine\")\n",
        "tokenized_sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hC11u5e-X35m",
        "outputId": "8d4285ce-40cc-4b19-9b40-bb893d4ac7e5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['[CLS]',\n",
              " 'o',\n",
              " '##h',\n",
              " 'i',\n",
              " '##t',\n",
              " '[UNK]',\n",
              " '[UNK]',\n",
              " 'f',\n",
              " '##i',\n",
              " '##n',\n",
              " '##e',\n",
              " '[SEP]']"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenized_sentence.tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0f5gF5m4X5m9",
        "outputId": "f9d7a515-af67-4805-b756-a25f1169cea2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Encoding(num_tokens=24, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenized_sentence = tokenizer1.encode(\"ohoh i thougt it might be workingg well\")\n",
        "tokenized_sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ijHQcQ4gLkpB",
        "outputId": "46a6fd97-1ea6-4c28-e006-d05982bd9be4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['[CLS]',\n",
              " 'o',\n",
              " '##h',\n",
              " '##o',\n",
              " '##h',\n",
              " 'i',\n",
              " 'th',\n",
              " '##o',\n",
              " '##u',\n",
              " '##g',\n",
              " '##t',\n",
              " 'i',\n",
              " '##t',\n",
              " 'm',\n",
              " '##i',\n",
              " '##gh',\n",
              " '##t',\n",
              " '[UNK]',\n",
              " '[UNK]',\n",
              " 'w',\n",
              " '##e',\n",
              " '##l',\n",
              " '##l',\n",
              " '[SEP]']"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenized_sentence.tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O_mmlJp04hwf",
        "outputId": "92d41f25-959e-43a3-8795-903b44a5e1e5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[01;34m./\u001b[0m\n",
            "├── \u001b[00mcorpus.txt\u001b[0m\n",
            "├── \u001b[00mIMDB.csv\u001b[0m\n",
            "└── \u001b[01;34mtokenizer\u001b[0m\n",
            "    └── \u001b[00mvocab.txt\u001b[0m\n",
            "\n",
            "1 directory, 3 files\n"
          ]
        }
      ],
      "source": [
        "!tree -a \"./\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RGFja_cXuLU0"
      },
      "source": [
        "## 3 训练分词器\n",
        "\n",
        "1. 将原始数据文件 (IMDB.csv) 重新组织为 corpus.txt 文件\n",
        "2. 利用 corpus.txt 数据进行分词, 生成 vocab.txt\n",
        "3. 利用 vocab.txt 训练分词器\n",
        "4. 利用 corpus.txt 生成数据集\n",
        "5. 生成 PyTorch DataLoaders\n",
        "6. 训练参数\n",
        "7. 模型配置\n",
        "8. 模型结构\n",
        "9. 训练\n",
        "10. 保存模型"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dh9p-wwI5gEC",
        "outputId": "2a9b3e51-b84b-48b2-d062-03240f4986d2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[01;34m./\u001b[0m\n",
            "├── \u001b[00mcorpus.txt\u001b[0m\n",
            "├── \u001b[00mIMDB.csv\u001b[0m\n",
            "└── \u001b[01;34mtokenizer\u001b[0m\n",
            "    └── \u001b[00mvocab.txt\u001b[0m\n",
            "\n",
            "1 directory, 3 files\n"
          ]
        }
      ],
      "source": [
        "!tree -a \"./\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z7-Nt9n2YA8E",
        "outputId": "9dab1fab-7cd8-4951-c1d6-bc6724c4093a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "BertTokenizerFast(name_or_path='tokenizer', vocab_size=63, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True, added_tokens_decoder={\n",
              "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t1: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t2: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t3: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t4: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "}\n",
              ")"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import BertTokenizerFast\n",
        "\n",
        "tokenizer2 = BertTokenizerFast.from_pretrained(\"tokenizer\")\n",
        "tokenizer2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x133t33yUzE6",
        "outputId": "ceec4fd2-5ee6-4027-b185-794f3e60fb1c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'input_ids': [2, 17, 39, 30, 39, 13, 46, 30, 37, 40, 28, 13, 28, 15, 29, 57, 28, 1, 1, 24, 26, 38, 38, 3], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer2(\"ohoh i thougt it might be workingg well\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LsNKPs76W0U-",
        "outputId": "e1ba318f-fde5-4666-e924-591bf56304b3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('tokenizer2/tokenizer_config.json',\n",
              " 'tokenizer2/special_tokens_map.json',\n",
              " 'tokenizer2/vocab.txt',\n",
              " 'tokenizer2/added_tokens.json',\n",
              " 'tokenizer2/tokenizer.json')"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer2.save_pretrained(\"tokenizer2\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y7KVN0m0W0YA",
        "outputId": "01c458a7-b204-47c4-9ab3-0e55c728dadc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[01;34m./\u001b[0m\n",
            "├── \u001b[00mcorpus.txt\u001b[0m\n",
            "├── \u001b[00mIMDB.csv\u001b[0m\n",
            "├── \u001b[01;34mtokenizer\u001b[0m\n",
            "│   └── \u001b[00mvocab.txt\u001b[0m\n",
            "└── \u001b[01;34mtokenizer2\u001b[0m\n",
            "    ├── \u001b[00mspecial_tokens_map.json\u001b[0m\n",
            "    ├── \u001b[00mtokenizer_config.json\u001b[0m\n",
            "    ├── \u001b[00mtokenizer.json\u001b[0m\n",
            "    └── \u001b[00mvocab.txt\u001b[0m\n",
            "\n",
            "2 directories, 7 files\n"
          ]
        }
      ],
      "source": [
        "!tree -a ./"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dazonQ5Pubbf"
      },
      "source": [
        "## 4 数据集\n",
        "\n",
        "1. 将原始数据文件 (IMDB.csv) 重新组织为 corpus.txt 文件\n",
        "2. 利用 corpus.txt 数据进行分词, 生成 vocab.txt\n",
        "3. 利用 vocab.txt 训练分词器\n",
        "4. 利用 corpus.txt 生成数据集\n",
        "5. 生成 PyTorch DataLoaders\n",
        "6. 训练参数\n",
        "7. 模型配置\n",
        "8. 模型结构\n",
        "9. 训练\n",
        "10. 保存模型"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xHwTfLvKYGKA",
        "outputId": "6437fee9-7b6b-4079-c0d2-9488b8719d8a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/data/datasets/language_modeling.py:119: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the 🤗 Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<transformers.data.datasets.language_modeling.LineByLineTextDataset at 0x7a3188856950>"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import LineByLineTextDataset\n",
        "\n",
        "dataset = LineByLineTextDataset(\n",
        "    tokenizer=tokenizer2, file_path=\"corpus.txt\", block_size=128\n",
        ")\n",
        "# dir(dataset)\n",
        "# dataset.examples\n",
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KdwWGt-yunQH",
        "outputId": "9084b967-9a2d-4f40-bdbb-ee7eb95e16ff"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'input_ids': tensor([ 2, 17, 27, 26, 17, 36, 58, 17, 28, 39, 47, 19, 26, 32, 29, 26, 33, 47,\n",
              "         35, 12, 41, 35, 15, 49, 59, 26, 31, 46, 41, 28,  5,  5,  5,  3])}"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_2lNTanmuuVw"
      },
      "source": [
        "## 5 PyTorch DataLoaders\n",
        "\n",
        "1. 将原始数据文件 (IMDB.csv) 重新组织为 corpus.txt 文件\n",
        "2. 利用 corpus.txt 数据进行分词, 生成 vocab.txt\n",
        "3. 利用 vocab.txt 训练分词器\n",
        "4. 利用 corpus.txt 生成数据集\n",
        "5. 生成 PyTorch DataLoaders\n",
        "6. 训练参数\n",
        "7. 模型配置\n",
        "8. 模型结构\n",
        "9. 训练\n",
        "10. 保存模型"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "njU0BD1UYMVE",
        "outputId": "81e0fe14-2e78-4a9a-d64a-50bb67268acc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DataCollatorForLanguageModeling(tokenizer=BertTokenizerFast(name_or_path='tokenizer', vocab_size=63, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True, added_tokens_decoder={\n",
              "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t1: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t2: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t3: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t4: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "}\n",
              "), mlm=True, mlm_probability=0.15, pad_to_multiple_of=None, tf_experimental_compile=False, return_tensors='pt')"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import DataCollatorForLanguageModeling\n",
        "\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer2, mlm=True, mlm_probability=0.15\n",
        ")\n",
        "data_collator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NCvFAGQj0IFu"
      },
      "source": [
        "## 6 TrainingArguments\n",
        "\n",
        "1. 将原始数据文件 (IMDB.csv) 重新组织为 corpus.txt 文件\n",
        "2. 利用 corpus.txt 数据进行分词, 生成 vocab.txt\n",
        "3. 利用 vocab.txt 训练分词器\n",
        "4. 利用 corpus.txt 生成数据集\n",
        "5. 生成 PyTorch DataLoaders\n",
        "6. 训练参数\n",
        "7. 模型配置\n",
        "8. 模型结构\n",
        "9. 训练\n",
        "10. 保存模型"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OBM3SvrHYTNH",
        "outputId": "c694bc8a-965d-4f2c-cba9-b5dbfe69ac25"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "TrainingArguments(\n",
              "_n_gpu=0,\n",
              "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n",
              "adafactor=False,\n",
              "adam_beta1=0.9,\n",
              "adam_beta2=0.999,\n",
              "adam_epsilon=1e-08,\n",
              "auto_find_batch_size=False,\n",
              "average_tokens_across_devices=False,\n",
              "batch_eval_metrics=False,\n",
              "bf16=False,\n",
              "bf16_full_eval=False,\n",
              "data_seed=None,\n",
              "dataloader_drop_last=False,\n",
              "dataloader_num_workers=0,\n",
              "dataloader_persistent_workers=False,\n",
              "dataloader_pin_memory=True,\n",
              "dataloader_prefetch_factor=None,\n",
              "ddp_backend=None,\n",
              "ddp_broadcast_buffers=None,\n",
              "ddp_bucket_cap_mb=None,\n",
              "ddp_find_unused_parameters=None,\n",
              "ddp_timeout=1800,\n",
              "debug=[],\n",
              "deepspeed=None,\n",
              "disable_tqdm=False,\n",
              "dispatch_batches=None,\n",
              "do_eval=False,\n",
              "do_predict=False,\n",
              "do_train=False,\n",
              "eval_accumulation_steps=None,\n",
              "eval_delay=0,\n",
              "eval_do_concat_batches=True,\n",
              "eval_on_start=False,\n",
              "eval_steps=None,\n",
              "eval_strategy=IntervalStrategy.NO,\n",
              "eval_use_gather_object=False,\n",
              "evaluation_strategy=None,\n",
              "fp16=False,\n",
              "fp16_backend=auto,\n",
              "fp16_full_eval=False,\n",
              "fp16_opt_level=O1,\n",
              "fsdp=[],\n",
              "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
              "fsdp_min_num_params=0,\n",
              "fsdp_transformer_layer_cls_to_wrap=None,\n",
              "full_determinism=False,\n",
              "gradient_accumulation_steps=1,\n",
              "gradient_checkpointing=False,\n",
              "gradient_checkpointing_kwargs=None,\n",
              "greater_is_better=None,\n",
              "group_by_length=False,\n",
              "half_precision_backend=auto,\n",
              "hub_always_push=False,\n",
              "hub_model_id=None,\n",
              "hub_private_repo=None,\n",
              "hub_strategy=HubStrategy.EVERY_SAVE,\n",
              "hub_token=<HUB_TOKEN>,\n",
              "ignore_data_skip=False,\n",
              "include_for_metrics=[],\n",
              "include_inputs_for_metrics=False,\n",
              "include_num_input_tokens_seen=False,\n",
              "include_tokens_per_second=False,\n",
              "jit_mode_eval=False,\n",
              "label_names=None,\n",
              "label_smoothing_factor=0.0,\n",
              "learning_rate=5e-05,\n",
              "length_column_name=length,\n",
              "load_best_model_at_end=False,\n",
              "local_rank=0,\n",
              "log_level=passive,\n",
              "log_level_replica=warning,\n",
              "log_on_each_node=True,\n",
              "logging_dir=checkout_point/runs/Feb27_03-10-33_8aed3e07fd1c,\n",
              "logging_first_step=False,\n",
              "logging_nan_inf_filter=True,\n",
              "logging_steps=500,\n",
              "logging_strategy=IntervalStrategy.STEPS,\n",
              "lr_scheduler_kwargs={},\n",
              "lr_scheduler_type=SchedulerType.LINEAR,\n",
              "max_grad_norm=1.0,\n",
              "max_steps=-1,\n",
              "metric_for_best_model=None,\n",
              "mp_parameters=,\n",
              "neftune_noise_alpha=None,\n",
              "no_cuda=False,\n",
              "num_train_epochs=1,\n",
              "optim=OptimizerNames.ADAMW_TORCH,\n",
              "optim_args=None,\n",
              "optim_target_modules=None,\n",
              "output_dir=checkout_point,\n",
              "overwrite_output_dir=True,\n",
              "past_index=-1,\n",
              "per_device_eval_batch_size=8,\n",
              "per_device_train_batch_size=128,\n",
              "prediction_loss_only=False,\n",
              "push_to_hub=False,\n",
              "push_to_hub_model_id=None,\n",
              "push_to_hub_organization=None,\n",
              "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
              "ray_scope=last,\n",
              "remove_unused_columns=True,\n",
              "report_to=[],\n",
              "restore_callback_states_from_checkpoint=False,\n",
              "resume_from_checkpoint=None,\n",
              "run_name=checkout_point,\n",
              "save_on_each_node=False,\n",
              "save_only_model=False,\n",
              "save_safetensors=True,\n",
              "save_steps=500,\n",
              "save_strategy=SaveStrategy.STEPS,\n",
              "save_total_limit=None,\n",
              "seed=42,\n",
              "skip_memory_metrics=True,\n",
              "split_batches=None,\n",
              "tf32=None,\n",
              "torch_compile=False,\n",
              "torch_compile_backend=None,\n",
              "torch_compile_mode=None,\n",
              "torch_empty_cache_steps=None,\n",
              "torchdynamo=None,\n",
              "tpu_metrics_debug=False,\n",
              "tpu_num_cores=None,\n",
              "use_cpu=False,\n",
              "use_ipex=False,\n",
              "use_legacy_prediction_loop=False,\n",
              "use_liger_kernel=False,\n",
              "use_mps_device=False,\n",
              "warmup_ratio=0.0,\n",
              "warmup_steps=0,\n",
              "weight_decay=0.0,\n",
              ")"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"checkout_point\",\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=1,\n",
        "    per_device_train_batch_size=128,\n",
        "    report_to=[]\n",
        ")\n",
        "training_args"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H1zU_ENj0IFu"
      },
      "source": [
        "## 7 BertConfig\n",
        "\n",
        "1. 将原始数据文件 (IMDB.csv) 重新组织为 corpus.txt 文件\n",
        "2. 利用 corpus.txt 数据进行分词, 生成 vocab.txt\n",
        "3. 利用 vocab.txt 训练分词器\n",
        "4. 利用 corpus.txt 生成数据集\n",
        "5. 生成 PyTorch DataLoaders\n",
        "6. 训练参数\n",
        "7. 模型配置\n",
        "8. 模型结构\n",
        "9. 训练\n",
        "10. 保存模型"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_u5_N_-9YYqw",
        "outputId": "8a806745-ad37-4c15-95ed-b98edae7f850"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "BertConfig {\n",
              "  \"attention_probs_dropout_prob\": 0.1,\n",
              "  \"classifier_dropout\": null,\n",
              "  \"hidden_act\": \"gelu\",\n",
              "  \"hidden_dropout_prob\": 0.1,\n",
              "  \"hidden_size\": 768,\n",
              "  \"initializer_range\": 0.02,\n",
              "  \"intermediate_size\": 3072,\n",
              "  \"layer_norm_eps\": 1e-12,\n",
              "  \"max_position_embeddings\": 512,\n",
              "  \"model_type\": \"bert\",\n",
              "  \"num_attention_heads\": 12,\n",
              "  \"num_hidden_layers\": 12,\n",
              "  \"pad_token_id\": 0,\n",
              "  \"position_embedding_type\": \"absolute\",\n",
              "  \"transformers_version\": \"4.48.3\",\n",
              "  \"type_vocab_size\": 2,\n",
              "  \"use_cache\": true,\n",
              "  \"vocab_size\": 30522\n",
              "}"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import BertConfig, BertForMaskedLM\n",
        "config = BertConfig()\n",
        "config"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sX-uvTxE0IFv"
      },
      "source": [
        "## 8 模型结构\n",
        "\n",
        "1. 将原始数据文件 (IMDB.csv) 重新组织为 corpus.txt 文件\n",
        "2. 利用 corpus.txt 数据进行分词, 生成 vocab.txt\n",
        "3. 利用 vocab.txt 训练分词器\n",
        "4. 利用 corpus.txt 生成数据集\n",
        "5. 生成 PyTorch DataLoaders\n",
        "6. 训练参数\n",
        "7. 模型配置\n",
        "8. 模型结构\n",
        "9. 训练\n",
        "10. 保存模型"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pcgUQ9G5Vuhf",
        "outputId": "86966515-ae0f-406b-974f-ba00988db44f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "BertForMaskedLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
            "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
            "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
            "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "BertForMaskedLM(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSdpaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (cls): BertOnlyMLMHead(\n",
              "    (predictions): BertLMPredictionHead(\n",
              "      (transform): BertPredictionHeadTransform(\n",
              "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (transform_act_fn): GELUActivation()\n",
              "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      )\n",
              "      (decoder): Linear(in_features=768, out_features=30522, bias=True)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 模型没有使用任何已经训练好的权重, 参数都是随机初始化的\n",
        "bert = BertForMaskedLM(config)\n",
        "bert"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wqjKwc7_0IFv"
      },
      "source": [
        "## 9 Trainer\n",
        "\n",
        "1. 将原始数据文件 (IMDB.csv) 重新组织为 corpus.txt 文件\n",
        "2. 利用 corpus.txt 数据进行分词, 生成 vocab.txt\n",
        "3. 利用 vocab.txt 训练分词器\n",
        "4. 利用 corpus.txt 生成数据集\n",
        "5. 生成 PyTorch DataLoaders\n",
        "6. 训练参数\n",
        "7. 模型配置\n",
        "8. 模型结构\n",
        "9. 训练\n",
        "10. 保存模型"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JQY0-ZLvYYtm",
        "outputId": "02405d21-66d3-456c-d5be-a1b2e711ebd0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<transformers.trainer.Trainer at 0x7a31601ce010>"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import Trainer\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=bert, args=training_args, data_collator=data_collator, train_dataset=dataset\n",
        ")\n",
        "trainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        },
        "id": "bVZDxhXBYYym",
        "outputId": "8a60eacf-fa8b-4c21-db83-11dbaef75e86"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1/1 00:20, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=1, training_loss=10.156402587890625, metrics={'train_runtime': 31.4171, 'train_samples_per_second': 0.127, 'train_steps_per_second': 0.032, 'total_flos': 69913778400.0, 'train_loss': 10.156402587890625, 'epoch': 1.0})"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fjzXoVyN0IFv"
      },
      "source": [
        "## 10 保存模型\n",
        "\n",
        "1. 将原始数据文件 (IMDB.csv) 重新组织为 corpus.txt 文件\n",
        "2. 利用 corpus.txt 数据进行分词, 生成 vocab.txt\n",
        "3. 利用 vocab.txt 训练分词器\n",
        "4. 利用 corpus.txt 生成数据集\n",
        "5. 生成 PyTorch DataLoaders\n",
        "6. 训练参数\n",
        "7. 模型配置\n",
        "8. 模型结构\n",
        "9. 训练\n",
        "10. 保存模型"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "6uzttHnJYY1C"
      },
      "outputs": [],
      "source": [
        "trainer.save_model(\"BertOut\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v6dCSGX3WWxd",
        "outputId": "aeb68ba7-bdaf-42dd-e461-f5c59bf04def"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[01;34m./\u001b[0m\n",
            "├── \u001b[01;34mBertOut\u001b[0m\n",
            "│   ├── \u001b[00mconfig.json\u001b[0m\n",
            "│   ├── \u001b[00mgeneration_config.json\u001b[0m\n",
            "│   ├── \u001b[00mmodel.safetensors\u001b[0m\n",
            "│   └── \u001b[00mtraining_args.bin\u001b[0m\n",
            "├── \u001b[01;34mcheckout_point\u001b[0m\n",
            "│   └── \u001b[01;34mcheckpoint-1\u001b[0m\n",
            "│       ├── \u001b[00mconfig.json\u001b[0m\n",
            "│       ├── \u001b[00mgeneration_config.json\u001b[0m\n",
            "│       ├── \u001b[00mmodel.safetensors\u001b[0m\n",
            "│       ├── \u001b[00moptimizer.pt\u001b[0m\n",
            "│       ├── \u001b[00mrng_state.pth\u001b[0m\n",
            "│       ├── \u001b[00mscheduler.pt\u001b[0m\n",
            "│       ├── \u001b[00mtrainer_state.json\u001b[0m\n",
            "│       └── \u001b[00mtraining_args.bin\u001b[0m\n",
            "├── \u001b[00mcorpus.txt\u001b[0m\n",
            "├── \u001b[00mIMDB.csv\u001b[0m\n",
            "├── \u001b[01;34mtokenizer\u001b[0m\n",
            "│   └── \u001b[00mvocab.txt\u001b[0m\n",
            "└── \u001b[01;34mtokenizer2\u001b[0m\n",
            "    ├── \u001b[00mspecial_tokens_map.json\u001b[0m\n",
            "    ├── \u001b[00mtokenizer_config.json\u001b[0m\n",
            "    ├── \u001b[00mtokenizer.json\u001b[0m\n",
            "    └── \u001b[00mvocab.txt\u001b[0m\n",
            "\n",
            "5 directories, 19 files\n"
          ]
        }
      ],
      "source": [
        "!tree -a \"./\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XhA5XnuJ0IFw"
      },
      "source": [
        "## 使用不同参数的 BertConfig 实例化模型并训练"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M-j4LqJHYs0F",
        "outputId": "d19c52b1-0cf1-4edb-962b-0d1d12ead04e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "BertConfig {\n",
              "  \"attention_probs_dropout_prob\": 0.1,\n",
              "  \"classifier_dropout\": null,\n",
              "  \"hidden_act\": \"gelu\",\n",
              "  \"hidden_dropout_prob\": 0.1,\n",
              "  \"hidden_size\": 768,\n",
              "  \"initializer_range\": 0.02,\n",
              "  \"intermediate_size\": 3072,\n",
              "  \"layer_norm_eps\": 1e-12,\n",
              "  \"max_position_embeddings\": 512,\n",
              "  \"model_type\": \"bert\",\n",
              "  \"num_attention_heads\": 12,\n",
              "  \"num_hidden_layers\": 12,\n",
              "  \"pad_token_id\": 0,\n",
              "  \"position_embedding_type\": \"absolute\",\n",
              "  \"transformers_version\": \"4.48.3\",\n",
              "  \"type_vocab_size\": 2,\n",
              "  \"use_cache\": true,\n",
              "  \"vocab_size\": 30522\n",
              "}"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import BertConfig\n",
        "\n",
        "config = BertConfig()\n",
        "config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MyY0_v-eYs2k",
        "outputId": "8b1ed074-4acc-4a14-e6cb-bb3ebf627d67"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "BertConfig {\n",
              "  \"attention_probs_dropout_prob\": 0.1,\n",
              "  \"classifier_dropout\": null,\n",
              "  \"hidden_act\": \"gelu\",\n",
              "  \"hidden_dropout_prob\": 0.1,\n",
              "  \"hidden_size\": 128,\n",
              "  \"initializer_range\": 0.02,\n",
              "  \"intermediate_size\": 512,\n",
              "  \"layer_norm_eps\": 1e-12,\n",
              "  \"max_position_embeddings\": 512,\n",
              "  \"model_type\": \"bert\",\n",
              "  \"num_attention_heads\": 2,\n",
              "  \"num_hidden_layers\": 2,\n",
              "  \"pad_token_id\": 0,\n",
              "  \"position_embedding_type\": \"absolute\",\n",
              "  \"transformers_version\": \"4.48.3\",\n",
              "  \"type_vocab_size\": 2,\n",
              "  \"use_cache\": true,\n",
              "  \"vocab_size\": 30522\n",
              "}"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tiny_bert_config = BertConfig(\n",
        "    max_position_embeddings=512,\n",
        "    hidden_size=128,\n",
        "    num_attention_heads=2,\n",
        "    num_hidden_layers=2,\n",
        "    intermediate_size=512,\n",
        ")\n",
        "tiny_bert_config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        },
        "id": "ByC2zBtwYs44",
        "outputId": "38954150-dda2-4b83-9147-e021237c2018"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1/1 00:00, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=1, training_loss=10.408717155456543, metrics={'train_runtime': 0.9756, 'train_samples_per_second': 4.1, 'train_steps_per_second': 1.025, 'total_flos': 362377440.0, 'train_loss': 10.408717155456543, 'epoch': 1.0})"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tiny_bert = BertForMaskedLM(tiny_bert_config)\n",
        "trainer = Trainer(\n",
        "    model=tiny_bert,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=dataset,\n",
        ")\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oPbGOOhu0IFx"
      },
      "source": [
        "## tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fMCmbDfFYs_s",
        "outputId": "8ea6b151-4e6b-4a09-d1e3-9621134a6e64"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "BertTokenizerFast(name_or_path='bert-base-uncased', vocab_size=30522, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
              "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "}\n",
              ")"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import TFBertModel, BertTokenizerFast\n",
        "\n",
        "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n",
        "tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176,
          "referenced_widgets": [
            "bae059a067cd48b28c4b8285c65ea61a",
            "36b0b5bc3bda4d849276448583588def",
            "501c3ce557304a98aedf065942d2dad3",
            "620dc444288547df9777cb9fd814099e",
            "496e5f16f0124820823df55d135d3569",
            "f36eca878e06466ba31a5da77e899ed1",
            "a7f8c2f160a4467eb9cced8c2f2b6ef4",
            "9f3c574b12c44cb3af7328f09a167f4f",
            "19b8cfc91a3743a6b616a0693971c44d",
            "07c7f0ea07e24d9ea14730fdc87c9350",
            "6c3358e44b8e4dda8cec219525c67b61"
          ]
        },
        "id": "-nEbVHMPWyxM",
        "outputId": "2a9c2a1b-3f1b-4a67-d2d8-1ead2b1f567d"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bae059a067cd48b28c4b8285c65ea61a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the weights of TFBertModel were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<transformers.models.bert.modeling_tf_bert.TFBertModel at 0x7a3160d74590>"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "bert = TFBertModel.from_pretrained(\"bert-base-uncased\")\n",
        "bert"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zJfgm72-WyuL",
        "outputId": "9ae14fb2-93f6-40be-be5e-9ecb81c73d8b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[<transformers.models.bert.modeling_tf_bert.TFBertMainLayer at 0x7a315f6c7fd0>]"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "bert.layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z-tQO7EbYtCR",
        "outputId": "5ea29395-48aa-4254-f88a-b81cbaec552e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py:2681: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'input_ids': <tf.Tensor: shape=(2, 256), dtype=int32, numpy=\n",
              "array([[  101,  7592,  2129,  2003,  2009,  2183,  2007,  2017,   102,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0],\n",
              "       [  101, 11082,  3231,  2009,   102,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0]], dtype=int32)>, 'token_type_ids': <tf.Tensor: shape=(2, 256), dtype=int32, numpy=\n",
              "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(2, 256), dtype=int32, numpy=\n",
              "array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              "       [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=int32)>}"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenized_text = tokenizer.batch_encode_plus(\n",
        "    [\"hello how is it going with you\", \"lets test it\"],\n",
        "    return_tensors=\"tf\",\n",
        "    max_length=256,\n",
        "    truncation=True,\n",
        "    pad_to_max_length=True,\n",
        ")\n",
        "tokenized_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xiZoc257-MLC",
        "outputId": "745e3df6-1b57-4439-d6cf-fb04bc3db347"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "TFBaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=<tf.Tensor: shape=(2, 256, 768), dtype=float32, numpy=\n",
              "array([[[ 1.00471482e-01,  6.77027702e-02, -8.33593458e-02, ...,\n",
              "         -4.93304521e-01,  1.16539404e-01,  2.26647347e-01],\n",
              "        [ 3.23624790e-01,  3.70718509e-01,  6.14685833e-01, ...,\n",
              "         -6.27267718e-01,  3.79083008e-01,  7.05303252e-02],\n",
              "        [ 1.99533507e-01, -8.75509262e-01, -6.47860318e-02, ...,\n",
              "         -1.28087141e-02,  3.07651460e-01, -2.07329299e-02],\n",
              "        ...,\n",
              "        [-6.53300136e-02,  1.19046159e-01,  5.76847076e-01, ...,\n",
              "         -2.95460641e-01,  2.49744691e-02,  1.13964200e-01],\n",
              "        [-2.64715314e-01, -7.86391348e-02,  5.47280669e-01, ...,\n",
              "         -1.37515366e-01, -5.94692305e-02, -5.17934039e-02],\n",
              "        [-2.44958907e-01, -1.14799649e-01,  5.92174232e-01, ...,\n",
              "         -1.56881928e-01, -3.39757986e-02, -8.46138969e-02]],\n",
              "\n",
              "       [[ 2.94559058e-02,  2.30868489e-01,  2.92651713e-01, ...,\n",
              "         -1.30421668e-01,  1.89659417e-01,  4.68428314e-01],\n",
              "        [ 1.70523179e+00,  6.91360354e-01,  7.31510639e-01, ...,\n",
              "          2.89302945e-01,  5.36760628e-01, -1.54551670e-01],\n",
              "        [ 1.04597472e-01,  9.63682979e-02,  6.99661300e-02, ...,\n",
              "         -4.15922523e-01, -1.18989803e-01, -6.72239900e-01],\n",
              "        ...,\n",
              "        [ 8.00910175e-01,  2.38983303e-01,  4.15492892e-01, ...,\n",
              "          3.90530005e-02,  2.34373450e-01,  1.22278214e-01],\n",
              "        [ 2.60862529e-01,  4.43281680e-02,  3.63647580e-01, ...,\n",
              "         -7.54356384e-04,  3.84612381e-02, -2.14212552e-01],\n",
              "        [-2.30111539e-01, -4.98388231e-01, -1.26493275e-02, ...,\n",
              "          4.49867755e-01,  6.16018549e-02, -2.61357427e-01]]],\n",
              "      dtype=float32)>, pooler_output=<tf.Tensor: shape=(2, 768), dtype=float32, numpy=\n",
              "array([[-0.9204854 , -0.37139004, -0.6051263 , ..., -0.44737005,\n",
              "        -0.643476  ,  0.94232714],\n",
              "       [-0.8854158 , -0.26547667,  0.21014975, ...,  0.17237106,\n",
              "        -0.64029896,  0.8888343 ]], dtype=float32)>, past_key_values=None, hidden_states=None, attentions=None, cross_attentions=None)"
            ]
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "bert(tokenized_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DMmRg2X-Y7LF",
        "outputId": "51d89ff0-61f5-4fc9-8854-c83e3b7b271c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tf_keras.src.engine.functional.Functional at 0x7a314acc2350>"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from tensorflow import keras\n",
        "import tensorflow as tf\n",
        "\n",
        "max_length = 256\n",
        "tokens = keras.layers.Input(shape=(max_length,), dtype=tf.dtypes.int32)\n",
        "masks = keras.layers.Input(shape=(max_length,), dtype=tf.dtypes.int32)\n",
        "embedding_layer = bert.layers[0]([tokens, masks])[0][:, 0, :]\n",
        "dense = tf.keras.layers.Dense(units=2, activation=\"softmax\")(embedding_layer)\n",
        "\n",
        "model = keras.Model([tokens, masks], dense)\n",
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T8xeS5TyY7No",
        "outputId": "c510f379-e0e9-46e5-af18-05840e67aea4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'input_ids': <tf.Tensor: shape=(2, 256), dtype=int32, numpy=\n",
              "array([[ 101, 7592, 2129, 2003, 2009, 2183, 2007, 2017,  102,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0],\n",
              "       [ 101, 7592, 2129, 2003, 2009, 2183, 2007, 2017,  102,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0]], dtype=int32)>, 'token_type_ids': <tf.Tensor: shape=(2, 256), dtype=int32, numpy=\n",
              "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(2, 256), dtype=int32, numpy=\n",
              "array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=int32)>}"
            ]
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenized = tokenizer.batch_encode_plus(\n",
        "    [\"hello how is it going with you\", \"hello how is it going with you\"],\n",
        "    return_tensors=\"tf\",\n",
        "    max_length=max_length,\n",
        "    truncation=True,\n",
        "    pad_to_max_length=True,\n",
        ")\n",
        "tokenized"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AGMwpG8OY7P_",
        "outputId": "272139ea-1204-465a-a85e-00b20a40411b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
              "array([[0.61480767, 0.38519228],\n",
              "       [0.61480767, 0.38519228]], dtype=float32)>"
            ]
          },
          "execution_count": 51,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model([tokenized[\"input_ids\"], tokenized[\"attention_mask\"]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b24ahO1UY7SR",
        "outputId": "0d36d272-b966-4ecc-86ff-1bdfe2ce4cbc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)        [(None, 256)]                0         []                            \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)        [(None, 256)]                0         []                            \n",
            "                                                                                                  \n",
            " bert (TFBertMainLayer)      TFBaseModelOutputWithPooli   1094822   ['input_1[0][0]',             \n",
            "                             ngAndCrossAttentions(last_   40         'input_2[0][0]']             \n",
            "                             hidden_state=(None, 256, 7                                           \n",
            "                             68),                                                                 \n",
            "                              pooler_output=(None, 768)                                           \n",
            "                             , past_key_values=None, hi                                           \n",
            "                             dden_states=None, attentio                                           \n",
            "                             ns=None, cross_attentions=                                           \n",
            "                             None)                                                                \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem (  (None, 768)                  0         ['bert[0][0]']                \n",
            " SlicingOpLambda)                                                                                 \n",
            "                                                                                                  \n",
            " dense (Dense)               (None, 2)                    1538      ['tf.__operators__.getitem[0][\n",
            "                                                                    0]']                          \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 109483778 (417.65 MB)\n",
            "Trainable params: 109483778 (417.65 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.compile(optimizer=\"Adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "XSCi31iUZL1Y"
      },
      "outputs": [],
      "source": [
        "model.layers[2].trainable = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wcfBTBw8ZL6V",
        "outputId": "676cdc81-e7f5-4523-db72-6a300e3d4392"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py:2681: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "imdb_df = pd.read_csv(\"IMDB.csv\")\n",
        "reviews = list(imdb_df.review)\n",
        "tokenized_reviews = tokenizer.batch_encode_plus(\n",
        "    reviews,\n",
        "    return_tensors=\"tf\",\n",
        "    max_length=max_length,\n",
        "    truncation=True,\n",
        "    pad_to_max_length=True,\n",
        ")\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "train_split = int(0.8 * len(tokenized_reviews[\"attention_mask\"]))\n",
        "train_tokens = tokenized_reviews[\"input_ids\"][:train_split]\n",
        "test_tokens = tokenized_reviews[\"input_ids\"][train_split:]\n",
        "train_masks = tokenized_reviews[\"attention_mask\"][:train_split]\n",
        "test_masks = tokenized_reviews[\"attention_mask\"][train_split:]\n",
        "sentiments = list(imdb_df.sentiment)\n",
        "labels = np.array(\n",
        "    [[0, 1] if sentiment == \"positive\" else [1, 0] for sentiment in sentiments]\n",
        ")\n",
        "train_labels = labels[:train_split]\n",
        "test_labels = labels[train_split:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YaQlIkdZZL8c",
        "outputId": "12f61d96-3deb-4734-bee6-40bc974c0487"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 67s 67s/step - loss: 1.5588 - accuracy: 0.0000e+00\n",
            "Epoch 2/5\n",
            "1/1 [==============================] - 11s 11s/step - loss: 3.9736e-08 - accuracy: 1.0000\n",
            "Epoch 3/5\n",
            "1/1 [==============================] - 10s 10s/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
            "Epoch 4/5\n",
            "1/1 [==============================] - 10s 10s/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
            "Epoch 5/5\n",
            "1/1 [==============================] - 11s 11s/step - loss: 0.0000e+00 - accuracy: 1.0000\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<tf_keras.src.callbacks.History at 0x7a315dd0de90>"
            ]
          },
          "execution_count": 55,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 模型没有任何参数, 从头开始训练\n",
        "model.fit([train_tokens, train_masks], train_labels, epochs=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3RTyDkLzZL-8",
        "outputId": "161f87d2-4525-45fc-9af3-6431a9de640d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[01;34m./\u001b[0m\n",
            "├── \u001b[01;34mBertOut\u001b[0m\n",
            "│   ├── \u001b[00mconfig.json\u001b[0m\n",
            "│   ├── \u001b[00mgeneration_config.json\u001b[0m\n",
            "│   ├── \u001b[00mmodel.safetensors\u001b[0m\n",
            "│   └── \u001b[00mtraining_args.bin\u001b[0m\n",
            "├── \u001b[01;34mcheckout_point\u001b[0m\n",
            "│   └── \u001b[01;34mcheckpoint-1\u001b[0m\n",
            "│       ├── \u001b[00mconfig.json\u001b[0m\n",
            "│       ├── \u001b[00mgeneration_config.json\u001b[0m\n",
            "│       ├── \u001b[00mmodel.safetensors\u001b[0m\n",
            "│       ├── \u001b[00moptimizer.pt\u001b[0m\n",
            "│       ├── \u001b[00mrng_state.pth\u001b[0m\n",
            "│       ├── \u001b[00mscheduler.pt\u001b[0m\n",
            "│       ├── \u001b[00mtrainer_state.json\u001b[0m\n",
            "│       └── \u001b[00mtraining_args.bin\u001b[0m\n",
            "├── \u001b[00mcorpus.txt\u001b[0m\n",
            "├── \u001b[00mIMDB.csv\u001b[0m\n",
            "├── \u001b[01;34mtokenizer\u001b[0m\n",
            "│   └── \u001b[00mvocab.txt\u001b[0m\n",
            "└── \u001b[01;34mtokenizer2\u001b[0m\n",
            "    ├── \u001b[00mspecial_tokens_map.json\u001b[0m\n",
            "    ├── \u001b[00mtokenizer_config.json\u001b[0m\n",
            "    ├── \u001b[00mtokenizer.json\u001b[0m\n",
            "    └── \u001b[00mvocab.txt\u001b[0m\n",
            "\n",
            "5 directories, 19 files\n"
          ]
        }
      ],
      "source": [
        "!tree -a \"./\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9DiAg6xEZekh"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "07c7f0ea07e24d9ea14730fdc87c9350": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "19b8cfc91a3743a6b616a0693971c44d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "36b0b5bc3bda4d849276448583588def": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f36eca878e06466ba31a5da77e899ed1",
            "placeholder": "​",
            "style": "IPY_MODEL_a7f8c2f160a4467eb9cced8c2f2b6ef4",
            "value": "model.safetensors: 100%"
          }
        },
        "496e5f16f0124820823df55d135d3569": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "501c3ce557304a98aedf065942d2dad3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9f3c574b12c44cb3af7328f09a167f4f",
            "max": 440449768,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_19b8cfc91a3743a6b616a0693971c44d",
            "value": 440449768
          }
        },
        "620dc444288547df9777cb9fd814099e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_07c7f0ea07e24d9ea14730fdc87c9350",
            "placeholder": "​",
            "style": "IPY_MODEL_6c3358e44b8e4dda8cec219525c67b61",
            "value": " 440M/440M [00:02&lt;00:00, 146MB/s]"
          }
        },
        "6c3358e44b8e4dda8cec219525c67b61": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9f3c574b12c44cb3af7328f09a167f4f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a7f8c2f160a4467eb9cced8c2f2b6ef4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bae059a067cd48b28c4b8285c65ea61a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_36b0b5bc3bda4d849276448583588def",
              "IPY_MODEL_501c3ce557304a98aedf065942d2dad3",
              "IPY_MODEL_620dc444288547df9777cb9fd814099e"
            ],
            "layout": "IPY_MODEL_496e5f16f0124820823df55d135d3569"
          }
        },
        "f36eca878e06466ba31a5da77e899ed1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
